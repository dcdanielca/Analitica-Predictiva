{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transfer_learning.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"jMD1eX-EaL-N","colab_type":"code","outputId":"4c61a9ea-fbe5-40c1-b24a-56289a2aa6d6","executionInfo":{"status":"ok","timestamp":1562269024357,"user_tz":300,"elapsed":3248,"user":{"displayName":"Carlos Andres Madrigal Gonzalez","photoUrl":"","userId":"09724702270983160783"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lo6S1xOlCWxQ","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"/content/drive/My Drive/UNAL/Cátedra/Predictive Analytics/Clases/Ejercicios/NoteBook_TransferLearning (1)/NoteBook_TransferLearning/\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iv9XhxdFg3aN","colab_type":"code","outputId":"800da2cf-c3f4-4822-84f7-151720f6a153","executionInfo":{"status":"ok","timestamp":1562269032014,"user_tz":300,"elapsed":1451,"user":{"displayName":"Carlos Andres Madrigal Gonzalez","photoUrl":"","userId":"09724702270983160783"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!ls"],"execution_count":13,"outputs":[{"output_type":"stream","text":["imagen.jpg  transfer_learning.ipynb\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QyB8PIMwGgcN","colab_type":"text"},"source":["En los ejemplos realizados en este cuaderno se utilizaran modelos preentrenados. Estos modelos y las herramenitas para modificarlos se encuentran en las librerias TENSORNETS creadas por Taehoon Lee. Estas se pueden encontrar en el repositorio:\n","https://github.com/taehoonlee/tensornets\n","para instalar estas librerias se requiere cython por lo que se instalan los dos en la siguiente celda"]},{"cell_type":"code","metadata":{"id":"CPytUcG9Cotb","colab_type":"code","outputId":"92ef1bba-95a3-4e15-9706-0b088f0b6894","executionInfo":{"status":"ok","timestamp":1562269041502,"user_tz":300,"elapsed":5553,"user":{"displayName":"Carlos Andres Madrigal Gonzalez","photoUrl":"","userId":"09724702270983160783"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["!pip install cython\n","!pip install tensornets"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (0.29.10)\n","Requirement already satisfied: tensornets in /usr/local/lib/python3.6/dist-packages (0.4.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"92sknewTR4-f","colab_type":"text"},"source":["En el siguiente ejemplo se muestra como cargar una arquitectura pre-entrenada para usarla con un dato nuevo."]},{"cell_type":"code","metadata":{"id":"0AfL_3lrFPNU","colab_type":"code","outputId":"0d988379-8b1c-438e-9e79-9c8038c2517c","executionInfo":{"status":"ok","timestamp":1562269058712,"user_tz":300,"elapsed":13603,"user":{"displayName":"Carlos Andres Madrigal Gonzalez","photoUrl":"","userId":"09724702270983160783"}},"colab":{"base_uri":"https://localhost:8080/","height":160}},"source":["\n","import tensorflow as tf\n","import tensornets as nets\n","\n","\n","tf.reset_default_graph()\n","\n","inputs = tf.placeholder(tf.float32, [None, 224, 224, 3])\n","model = nets.ResNet50(inputs)\n","assert isinstance(model, tf.Tensor)\n","\n","img = nets.utils.load_img('imagen.jpg', target_size=256, crop_size=224)\n","assert img.shape == (1, 224, 224, 3)\n","\n","with tf.Session() as sess:\n","    img = model.preprocess(img)  # equivalent to img = nets.preprocess(model, img)\n","    sess.run(model.pretrained())  # equivalent to nets.pretrained(model)\n","    preds = sess.run(model, {inputs: img})\n","\n","print(nets.utils.decode_predictions(preds, top=2)[0])"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Downloading data from https://github.com/taehoonlee/deep-learning-models/releases/download/resnet/resnet50.h5\n","102899712/102891672 [==============================] - 1s 0us/step\n"],"name":"stdout"},{"output_type":"stream","text":["W0704 19:37:31.022450 140440294532992 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensornets/utils.py:130: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n","40960/35363 [==================================] - 0s 0us/step\n","[('n02099601', 'golden_retriever', 0.6492632), ('n02108551', 'Tibetan_mastiff', 0.05680773)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J3-QKKHVSUCl","colab_type":"text"},"source":["Pero si lo que queremos es usar una salida intermedia de la arquitectura preentrenada para entrenar el resto con nuestros datos seguimos el siguiente procedimiento usando las funciones  get_middles() y get_outputs():"]},{"cell_type":"code","metadata":{"id":"geUiy_FUSu02","colab_type":"code","outputId":"a7667211-180d-4fec-f596-9ea7639c0673","executionInfo":{"status":"ok","timestamp":1562269265094,"user_tz":300,"elapsed":5382,"user":{"displayName":"Carlos Andres Madrigal Gonzalez","photoUrl":"","userId":"09724702270983160783"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import tensorflow as tf\n","import tensornets as nets\n","\n","tf.reset_default_graph()\n","\n","inputs = tf.placeholder(tf.float32, [None, 224, 224, 3])\n","model = nets.ResNet50(inputs)\n","assert isinstance(model, tf.Tensor)\n","\n","img = nets.utils.load_img('imagen.jpg', target_size=256, crop_size=224)\n","assert img.shape == (1, 224, 224, 3)\n","\n","with tf.Session() as sess:\n","    img = model.preprocess(img)\n","    sess.run(model.pretrained())\n","    middles = sess.run(model.get_middles(), {inputs: img})\n","    outputs = sess.run(model.get_outputs(), {inputs: img})\n","    \n","model.print_summary()\n","\n","model.print_middles()\n","assert middles[0].shape == (1, 56, 56, 256)\n","assert middles[-1].shape == (1, 7, 7, 2048)\n","\n","model.print_outputs()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Scope: resnet50\n","Total layers: 54\n","Total weights: 320\n","Total parameters: 25,636,712\n","Scope: resnet50\n","conv2/block1/out:0 (?, 56, 56, 256)\n","conv2/block2/out:0 (?, 56, 56, 256)\n","conv2/block3/out:0 (?, 56, 56, 256)\n","conv3/block1/out:0 (?, 28, 28, 512)\n","conv3/block2/out:0 (?, 28, 28, 512)\n","conv3/block3/out:0 (?, 28, 28, 512)\n","conv3/block4/out:0 (?, 28, 28, 512)\n","conv4/block1/out:0 (?, 14, 14, 1024)\n","conv4/block2/out:0 (?, 14, 14, 1024)\n","conv4/block3/out:0 (?, 14, 14, 1024)\n","conv4/block4/out:0 (?, 14, 14, 1024)\n","conv4/block5/out:0 (?, 14, 14, 1024)\n","conv4/block6/out:0 (?, 14, 14, 1024)\n","conv5/block1/out:0 (?, 7, 7, 2048)\n","conv5/block2/out:0 (?, 7, 7, 2048)\n","conv5/block3/out:0 (?, 7, 7, 2048)\n","Scope: resnet50\n","conv1/pad:0 (?, 230, 230, 3)\n","conv1/conv/BiasAdd:0 (?, 112, 112, 64)\n","conv1/bn/FusedBatchNorm:0 (?, 112, 112, 64)\n","conv1/relu:0 (?, 112, 112, 64)\n","pool1/pad:0 (?, 114, 114, 64)\n","pool1/MaxPool:0 (?, 56, 56, 64)\n","conv2/block1/0/conv/BiasAdd:0 (?, 56, 56, 256)\n","conv2/block1/0/bn/FusedBatchNorm:0 (?, 56, 56, 256)\n","conv2/block1/1/conv/BiasAdd:0 (?, 56, 56, 64)\n","conv2/block1/1/bn/FusedBatchNorm:0 (?, 56, 56, 64)\n","conv2/block1/1/relu:0 (?, 56, 56, 64)\n","conv2/block1/2/conv/BiasAdd:0 (?, 56, 56, 64)\n","conv2/block1/2/bn/FusedBatchNorm:0 (?, 56, 56, 64)\n","conv2/block1/2/relu:0 (?, 56, 56, 64)\n","conv2/block1/3/conv/BiasAdd:0 (?, 56, 56, 256)\n","conv2/block1/3/bn/FusedBatchNorm:0 (?, 56, 56, 256)\n","conv2/block1/out:0 (?, 56, 56, 256)\n","conv2/block2/1/conv/BiasAdd:0 (?, 56, 56, 64)\n","conv2/block2/1/bn/FusedBatchNorm:0 (?, 56, 56, 64)\n","conv2/block2/1/relu:0 (?, 56, 56, 64)\n","conv2/block2/2/conv/BiasAdd:0 (?, 56, 56, 64)\n","conv2/block2/2/bn/FusedBatchNorm:0 (?, 56, 56, 64)\n","conv2/block2/2/relu:0 (?, 56, 56, 64)\n","conv2/block2/3/conv/BiasAdd:0 (?, 56, 56, 256)\n","conv2/block2/3/bn/FusedBatchNorm:0 (?, 56, 56, 256)\n","conv2/block2/out:0 (?, 56, 56, 256)\n","conv2/block3/1/conv/BiasAdd:0 (?, 56, 56, 64)\n","conv2/block3/1/bn/FusedBatchNorm:0 (?, 56, 56, 64)\n","conv2/block3/1/relu:0 (?, 56, 56, 64)\n","conv2/block3/2/conv/BiasAdd:0 (?, 56, 56, 64)\n","conv2/block3/2/bn/FusedBatchNorm:0 (?, 56, 56, 64)\n","conv2/block3/2/relu:0 (?, 56, 56, 64)\n","conv2/block3/3/conv/BiasAdd:0 (?, 56, 56, 256)\n","conv2/block3/3/bn/FusedBatchNorm:0 (?, 56, 56, 256)\n","conv2/block3/out:0 (?, 56, 56, 256)\n","conv3/block1/0/conv/BiasAdd:0 (?, 28, 28, 512)\n","conv3/block1/0/bn/FusedBatchNorm:0 (?, 28, 28, 512)\n","conv3/block1/1/conv/BiasAdd:0 (?, 28, 28, 128)\n","conv3/block1/1/bn/FusedBatchNorm:0 (?, 28, 28, 128)\n","conv3/block1/1/relu:0 (?, 28, 28, 128)\n","conv3/block1/2/conv/BiasAdd:0 (?, 28, 28, 128)\n","conv3/block1/2/bn/FusedBatchNorm:0 (?, 28, 28, 128)\n","conv3/block1/2/relu:0 (?, 28, 28, 128)\n","conv3/block1/3/conv/BiasAdd:0 (?, 28, 28, 512)\n","conv3/block1/3/bn/FusedBatchNorm:0 (?, 28, 28, 512)\n","conv3/block1/out:0 (?, 28, 28, 512)\n","conv3/block2/1/conv/BiasAdd:0 (?, 28, 28, 128)\n","conv3/block2/1/bn/FusedBatchNorm:0 (?, 28, 28, 128)\n","conv3/block2/1/relu:0 (?, 28, 28, 128)\n","conv3/block2/2/conv/BiasAdd:0 (?, 28, 28, 128)\n","conv3/block2/2/bn/FusedBatchNorm:0 (?, 28, 28, 128)\n","conv3/block2/2/relu:0 (?, 28, 28, 128)\n","conv3/block2/3/conv/BiasAdd:0 (?, 28, 28, 512)\n","conv3/block2/3/bn/FusedBatchNorm:0 (?, 28, 28, 512)\n","conv3/block2/out:0 (?, 28, 28, 512)\n","conv3/block3/1/conv/BiasAdd:0 (?, 28, 28, 128)\n","conv3/block3/1/bn/FusedBatchNorm:0 (?, 28, 28, 128)\n","conv3/block3/1/relu:0 (?, 28, 28, 128)\n","conv3/block3/2/conv/BiasAdd:0 (?, 28, 28, 128)\n","conv3/block3/2/bn/FusedBatchNorm:0 (?, 28, 28, 128)\n","conv3/block3/2/relu:0 (?, 28, 28, 128)\n","conv3/block3/3/conv/BiasAdd:0 (?, 28, 28, 512)\n","conv3/block3/3/bn/FusedBatchNorm:0 (?, 28, 28, 512)\n","conv3/block3/out:0 (?, 28, 28, 512)\n","conv3/block4/1/conv/BiasAdd:0 (?, 28, 28, 128)\n","conv3/block4/1/bn/FusedBatchNorm:0 (?, 28, 28, 128)\n","conv3/block4/1/relu:0 (?, 28, 28, 128)\n","conv3/block4/2/conv/BiasAdd:0 (?, 28, 28, 128)\n","conv3/block4/2/bn/FusedBatchNorm:0 (?, 28, 28, 128)\n","conv3/block4/2/relu:0 (?, 28, 28, 128)\n","conv3/block4/3/conv/BiasAdd:0 (?, 28, 28, 512)\n","conv3/block4/3/bn/FusedBatchNorm:0 (?, 28, 28, 512)\n","conv3/block4/out:0 (?, 28, 28, 512)\n","conv4/block1/0/conv/BiasAdd:0 (?, 14, 14, 1024)\n","conv4/block1/0/bn/FusedBatchNorm:0 (?, 14, 14, 1024)\n","conv4/block1/1/conv/BiasAdd:0 (?, 14, 14, 256)\n","conv4/block1/1/bn/FusedBatchNorm:0 (?, 14, 14, 256)\n","conv4/block1/1/relu:0 (?, 14, 14, 256)\n","conv4/block1/2/conv/BiasAdd:0 (?, 14, 14, 256)\n","conv4/block1/2/bn/FusedBatchNorm:0 (?, 14, 14, 256)\n","conv4/block1/2/relu:0 (?, 14, 14, 256)\n","conv4/block1/3/conv/BiasAdd:0 (?, 14, 14, 1024)\n","conv4/block1/3/bn/FusedBatchNorm:0 (?, 14, 14, 1024)\n","conv4/block1/out:0 (?, 14, 14, 1024)\n","conv4/block2/1/conv/BiasAdd:0 (?, 14, 14, 256)\n","conv4/block2/1/bn/FusedBatchNorm:0 (?, 14, 14, 256)\n","conv4/block2/1/relu:0 (?, 14, 14, 256)\n","conv4/block2/2/conv/BiasAdd:0 (?, 14, 14, 256)\n","conv4/block2/2/bn/FusedBatchNorm:0 (?, 14, 14, 256)\n","conv4/block2/2/relu:0 (?, 14, 14, 256)\n","conv4/block2/3/conv/BiasAdd:0 (?, 14, 14, 1024)\n","conv4/block2/3/bn/FusedBatchNorm:0 (?, 14, 14, 1024)\n","conv4/block2/out:0 (?, 14, 14, 1024)\n","conv4/block3/1/conv/BiasAdd:0 (?, 14, 14, 256)\n","conv4/block3/1/bn/FusedBatchNorm:0 (?, 14, 14, 256)\n","conv4/block3/1/relu:0 (?, 14, 14, 256)\n","conv4/block3/2/conv/BiasAdd:0 (?, 14, 14, 256)\n","conv4/block3/2/bn/FusedBatchNorm:0 (?, 14, 14, 256)\n","conv4/block3/2/relu:0 (?, 14, 14, 256)\n","conv4/block3/3/conv/BiasAdd:0 (?, 14, 14, 1024)\n","conv4/block3/3/bn/FusedBatchNorm:0 (?, 14, 14, 1024)\n","conv4/block3/out:0 (?, 14, 14, 1024)\n","conv4/block4/1/conv/BiasAdd:0 (?, 14, 14, 256)\n","conv4/block4/1/bn/FusedBatchNorm:0 (?, 14, 14, 256)\n","conv4/block4/1/relu:0 (?, 14, 14, 256)\n","conv4/block4/2/conv/BiasAdd:0 (?, 14, 14, 256)\n","conv4/block4/2/bn/FusedBatchNorm:0 (?, 14, 14, 256)\n","conv4/block4/2/relu:0 (?, 14, 14, 256)\n","conv4/block4/3/conv/BiasAdd:0 (?, 14, 14, 1024)\n","conv4/block4/3/bn/FusedBatchNorm:0 (?, 14, 14, 1024)\n","conv4/block4/out:0 (?, 14, 14, 1024)\n","conv4/block5/1/conv/BiasAdd:0 (?, 14, 14, 256)\n","conv4/block5/1/bn/FusedBatchNorm:0 (?, 14, 14, 256)\n","conv4/block5/1/relu:0 (?, 14, 14, 256)\n","conv4/block5/2/conv/BiasAdd:0 (?, 14, 14, 256)\n","conv4/block5/2/bn/FusedBatchNorm:0 (?, 14, 14, 256)\n","conv4/block5/2/relu:0 (?, 14, 14, 256)\n","conv4/block5/3/conv/BiasAdd:0 (?, 14, 14, 1024)\n","conv4/block5/3/bn/FusedBatchNorm:0 (?, 14, 14, 1024)\n","conv4/block5/out:0 (?, 14, 14, 1024)\n","conv4/block6/1/conv/BiasAdd:0 (?, 14, 14, 256)\n","conv4/block6/1/bn/FusedBatchNorm:0 (?, 14, 14, 256)\n","conv4/block6/1/relu:0 (?, 14, 14, 256)\n","conv4/block6/2/conv/BiasAdd:0 (?, 14, 14, 256)\n","conv4/block6/2/bn/FusedBatchNorm:0 (?, 14, 14, 256)\n","conv4/block6/2/relu:0 (?, 14, 14, 256)\n","conv4/block6/3/conv/BiasAdd:0 (?, 14, 14, 1024)\n","conv4/block6/3/bn/FusedBatchNorm:0 (?, 14, 14, 1024)\n","conv4/block6/out:0 (?, 14, 14, 1024)\n","conv5/block1/0/conv/BiasAdd:0 (?, 7, 7, 2048)\n","conv5/block1/0/bn/FusedBatchNorm:0 (?, 7, 7, 2048)\n","conv5/block1/1/conv/BiasAdd:0 (?, 7, 7, 512)\n","conv5/block1/1/bn/FusedBatchNorm:0 (?, 7, 7, 512)\n","conv5/block1/1/relu:0 (?, 7, 7, 512)\n","conv5/block1/2/conv/BiasAdd:0 (?, 7, 7, 512)\n","conv5/block1/2/bn/FusedBatchNorm:0 (?, 7, 7, 512)\n","conv5/block1/2/relu:0 (?, 7, 7, 512)\n","conv5/block1/3/conv/BiasAdd:0 (?, 7, 7, 2048)\n","conv5/block1/3/bn/FusedBatchNorm:0 (?, 7, 7, 2048)\n","conv5/block1/out:0 (?, 7, 7, 2048)\n","conv5/block2/1/conv/BiasAdd:0 (?, 7, 7, 512)\n","conv5/block2/1/bn/FusedBatchNorm:0 (?, 7, 7, 512)\n","conv5/block2/1/relu:0 (?, 7, 7, 512)\n","conv5/block2/2/conv/BiasAdd:0 (?, 7, 7, 512)\n","conv5/block2/2/bn/FusedBatchNorm:0 (?, 7, 7, 512)\n","conv5/block2/2/relu:0 (?, 7, 7, 512)\n","conv5/block2/3/conv/BiasAdd:0 (?, 7, 7, 2048)\n","conv5/block2/3/bn/FusedBatchNorm:0 (?, 7, 7, 2048)\n","conv5/block2/out:0 (?, 7, 7, 2048)\n","conv5/block3/1/conv/BiasAdd:0 (?, 7, 7, 512)\n","conv5/block3/1/bn/FusedBatchNorm:0 (?, 7, 7, 512)\n","conv5/block3/1/relu:0 (?, 7, 7, 512)\n","conv5/block3/2/conv/BiasAdd:0 (?, 7, 7, 512)\n","conv5/block3/2/bn/FusedBatchNorm:0 (?, 7, 7, 512)\n","conv5/block3/2/relu:0 (?, 7, 7, 512)\n","conv5/block3/3/conv/BiasAdd:0 (?, 7, 7, 2048)\n","conv5/block3/3/bn/FusedBatchNorm:0 (?, 7, 7, 2048)\n","conv5/block3/out:0 (?, 7, 7, 2048)\n","avgpool:0 (?, 2048)\n","logits/BiasAdd:0 (?, 1000)\n","probs:0 (?, 1000)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q_O6Dbx2zrCz","colab_type":"text"},"source":["los valores almacenados en middles o en outputs, son salidas en capas intermedias de la arquitectura, y estas salidas se pueden usar como entradas para capas definidas por nosotros. Estas capas puede ser solo el MLP o Full conected al final de la arquitectura..."]},{"cell_type":"code","metadata":{"id":"RTIehYBRGCbI","colab_type":"code","colab":{}},"source":["import h5py\n","import matplotlib.pyplot as plt\n","import cv2\n","import numpy as np\n","import tensorflow as tf\n","import tensornets as nets\n","\n","\n","tf.reset_default_graph()\n","\n","# Parámetros para el entrenamiento\n","training_epochs=10\n","batch_size=10  \n","learning_rate = 0.001\n","display_step=1\n","\n","# Parametros de la red neuronal\n","n_hidden_1 = 1024 # 1st layer number of neurons\n","n_hidden_2 = 256 # 2nd layer number of neurons\n","n_input = 2048 # data input (feature shape ?,7,7,2048)\n","n_classes = 6 # total classes (6 signs)\n","\n","X = tf.placeholder(tf.float32, [None, 224, 224, 3])\n","X2 = tf.placeholder(tf.float32, [None, 7, 7, 2048])\n","Y = tf.placeholder(\"float\", [None, n_classes])\n","\n","# Para cargar el modelo con la libreria tensornets\n","model = nets.ResNet50(X)\n","assert isinstance(model, tf.Tensor)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6GhvEEWy0T0M","colab_type":"code","outputId":"65d5626f-18cb-4cab-a947-01814c21b146","executionInfo":{"status":"ok","timestamp":1562269569093,"user_tz":300,"elapsed":900,"user":{"displayName":"Carlos Andres Madrigal Gonzalez","photoUrl":"","userId":"09724702270983160783"}},"colab":{"base_uri":"https://localhost:8080/","height":500}},"source":["# Leer de una base de datos H5\n","with h5py.File('datasets/train_signs.h5','r') as h5data:\n","    ls=list(h5data.keys())\n","    print(ls)\n","    train_data=np.array(h5data.get('train_set_x')[:])\n","    train_labels=np.array(h5data.get('train_set_y')[:])\n","print(train_data.shape)    \n","print(train_labels.shape) \n","\n","with h5py.File('datasets/test_signs.h5','r') as h5data:\n","    ls=list(h5data.keys())\n","    print(ls)    \n","    test_data=np.array(h5data.get('test_set_x')[:])\n","    test_labels=np.array(h5data.get('test_set_y')[:])\n","print(test_data.shape)    \n","print(test_labels.shape)  \n","\n","# Funcion para cambiar de tamaño las imágenes para adpatarlas a la arquitectura\n","def resize_np (np_array):\n","    resized=[]\n","    for i in list(np_array):\n","        larger=cv2.resize(i,(224,224))\n","        resized.append(np.array(larger))\n","    return (np.array(resized).astype(np.float32))\n","\n","# Funcion para cambiar labels a onehot\n","def one_hot_transformation(labels,n_classes):\n","    samples=labels.size\n","    one_hot_labels=np.zeros((samples,n_classes))\n","    for i in range(samples):\n","        one_hot_labels[i,labels[i]]=1\n","    return(one_hot_labels)\n","\n","X_train=resize_np(train_data)\n","print(X_train.shape)\n","X_test=resize_np(test_data)\n","print(X_test.shape)\n","Y_train=one_hot_transformation(train_labels,n_classes)\n","print(Y_train.shape)\n","Y_test=one_hot_transformation(test_labels,n_classes)\n","print(Y_test.shape)\n","\n","# mostrar un ejemplo\n","plt.imshow(X_train[0])"],"execution_count":21,"outputs":[{"output_type":"stream","text":["['list_classes', 'train_set_x', 'train_set_y']\n","(1080, 64, 64, 3)\n","(1080,)\n","['list_classes', 'test_set_x', 'test_set_y']\n","(120, 64, 64, 3)\n","(120,)\n"],"name":"stdout"},{"output_type":"stream","text":["W0704 19:46:08.856271 140440294532992 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"stream","text":["(1080, 224, 224, 3)\n","(120, 224, 224, 3)\n","(1080, 6)\n","(120, 6)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fba6915e9b0>"]},"metadata":{"tags":[]},"execution_count":21},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAQIbWH/YTrLMAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"REFZ0Rk_SKAq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":142},"outputId":"7a8b73ca-3cca-4b15-9dc5-b15a7fa5e8cb","executionInfo":{"status":"ok","timestamp":1562269677066,"user_tz":300,"elapsed":975,"user":{"displayName":"Carlos Andres Madrigal Gonzalez","photoUrl":"","userId":"09724702270983160783"}}},"source":["# Declaración de los pesos y los bias (weight & bias)\n","weights = {'h1': tf.Variable(tf.truncated_normal([n_input, n_hidden_1],stddev=0.1)),\n","           'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2],stddev=0.1)),\n","           'out': tf.Variable(tf.truncated_normal([n_hidden_2, n_classes],stddev=0.1))\n","          }\n","biases = {'b1': tf.Variable(tf.truncated_normal([n_hidden_1],stddev=0.1)),\n","          'b2': tf.Variable(tf.truncated_normal([n_hidden_2],stddev=0.1)),\n","          'out': tf.Variable(tf.truncated_normal([n_classes],stddev=0.1))\n","         }\n","\n","# Definición del perceptrón multicapa\n","def multilayer_perceptron(x):\n","    pool = tf.nn.avg_pool(x, ksize=[1, 7, 7, 1], strides=[1, 1, 1, 1], padding='VALID')\n","    flat=tf.layers.flatten(pool)\n","    layer_1 = tf.add(tf.matmul(flat, weights['h1']), biases['b1']) # Hidden fully connected layer with 256 neurons\n","    relu_1=tf.nn.relu(layer_1)\n","    layer_2 = tf.add(tf.matmul(relu_1, weights['h2']), biases['b2']) # Hidden fully connected layer with 256 neurons\n","    relu_2=tf.nn.relu(layer_2)\n","    out_layer = tf.matmul(relu_2, weights['out']) + biases['out'] # Output fully connected layer with a neuron for each class\n","    return out_layer\n","# Declarar la operación que aplica el MLP usando la información de entrada\n","logits = multilayer_perceptron(X2)\n","# Declarar las operaciónes que establecen la funcion de perdida y optimización \n","# para el entrenamiento.\n","loss_op = tf.losses.softmax_cross_entropy(\n","    onehot_labels=Y,\n","    logits=logits,\n","    weights=1.0,\n","    scope=None,\n","    loss_collection=tf.GraphKeys.LOSSES,\n","    reduction=tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS\n",")\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","train_op = optimizer.minimize(loss_op)\n","# Initializing the variables\n","init = tf.global_variables_initializer()"],"execution_count":22,"outputs":[{"output_type":"stream","text":["W0704 19:47:56.371333 140440294532992 deprecation.py:323] From <ipython-input-22-7b7f323db323>:13: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","W0704 19:47:56.624610 140440294532992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"XL33mzkuRDIH","colab_type":"code","outputId":"c08d94ed-7bd8-4b74-d0d6-15438524fb42","executionInfo":{"status":"ok","timestamp":1562269743007,"user_tz":300,"elapsed":54342,"user":{"displayName":"Carlos Andres Madrigal Gonzalez","photoUrl":"","userId":"09724702270983160783"}},"colab":{"base_uri":"https://localhost:8080/","height":210}},"source":["with tf.Session() as sess:\n","    sess.run(init)\n","    sess.run(model.pretrained())  # equivalent to nets.pretrained(model)\n","    for epoch in range(training_epochs):\n","        avg_cost = 0.\n","        #obtiene el numero de grupos en que queda dividida la base de datos\n","        total_batch = int(Y_train.shape[0]/batch_size) \n","        # ciclo para entrenar con cada grupo de datos\n","        losses=[]\n","        for i in range(total_batch-1):\n","            batch_x= X_train[i*batch_size:(i+1)*batch_size]\n","            batch_y= Y_train[i*batch_size:(i+1)*batch_size]\n","            features = model.preprocess(batch_x)\n","            features = sess.run(model.get_middles(), {X: batch_x})[-1]\n","            \n","            # Correr la funcion de perdida y la operacion de optimización \n","            # con la respectiva alimentación del placeholder\n","            _,c =sess.run([train_op, loss_op],feed_dict={X2:features,Y:batch_y})\n","            # Promedio de resultados de la funcion de pérdida\n","            losses.append(c)\n","            avg_cost += c / total_batch\n","        # Mostrar el resultado del entrenamiento por grupos\n","        if epoch % display_step == 0:\n","            print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(avg_cost))\n","    print(\"Optimization Finished!\")"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Epoch: 0001 cost=1.269185369\n","Epoch: 0002 cost=0.157210293\n","Epoch: 0003 cost=0.093152052\n","Epoch: 0004 cost=0.038531400\n","Epoch: 0005 cost=0.005794533\n","Epoch: 0006 cost=0.002159547\n","Epoch: 0007 cost=0.000343002\n","Epoch: 0008 cost=0.000318316\n","Epoch: 0009 cost=0.000270541\n","Epoch: 0010 cost=0.000178472\n","Optimization Finished!\n"],"name":"stdout"}]}]}