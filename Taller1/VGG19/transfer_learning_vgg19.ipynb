{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transfer_learning_vgg19.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"jMD1eX-EaL-N","colab_type":"code","outputId":"9298482e-9f89-4d69-cb0a-0968e873fceb","executionInfo":{"status":"ok","timestamp":1563426664197,"user_tz":300,"elapsed":4380,"user":{"displayName":"Daniel Alexander Cano Cuartas","photoUrl":"https://lh5.googleusercontent.com/-4fBWDm-PEGk/AAAAAAAAAAI/AAAAAAAABLI/KrZmtZcFWbs/s64/photo.jpg","userId":"08439480702110199597"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lo6S1xOlCWxQ","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"/content/drive/My Drive/Analítica Predictiva/Taller1\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iv9XhxdFg3aN","colab_type":"code","outputId":"b6455b13-cd56-4b1b-caa2-b3eb72ca063b","executionInfo":{"status":"ok","timestamp":1563426670368,"user_tz":300,"elapsed":3064,"user":{"displayName":"Daniel Alexander Cano Cuartas","photoUrl":"https://lh5.googleusercontent.com/-4fBWDm-PEGk/AAAAAAAAAAI/AAAAAAAABLI/KrZmtZcFWbs/s64/photo.jpg","userId":"08439480702110199597"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["!ls"],"execution_count":3,"outputs":[{"output_type":"stream","text":["'Arquitectura desde 0.ipynb'\t    transfer_learning_vgg19.ipynb\n"," dataset_office.h5\t\t    usb_prueba.jpg\n"," transfer_learning_resnet50.ipynb\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QyB8PIMwGgcN","colab_type":"text"},"source":["En los ejemplos realizados en este cuaderno se utilizaran modelos preentrenados. Estos modelos y las herramenitas para modificarlos se encuentran en las librerias TENSORNETS creadas por Taehoon Lee. Estas se pueden encontrar en el repositorio:\n","https://github.com/taehoonlee/tensornets\n","para instalar estas librerias se requiere cython por lo que se instalan los dos en la siguiente celda"]},{"cell_type":"code","metadata":{"id":"CPytUcG9Cotb","colab_type":"code","outputId":"f7776539-dac1-4e73-8e63-1281be7c6ad8","executionInfo":{"status":"ok","timestamp":1563426679249,"user_tz":300,"elapsed":6737,"user":{"displayName":"Daniel Alexander Cano Cuartas","photoUrl":"https://lh5.googleusercontent.com/-4fBWDm-PEGk/AAAAAAAAAAI/AAAAAAAABLI/KrZmtZcFWbs/s64/photo.jpg","userId":"08439480702110199597"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!pip install cython\n","!pip install tensornets"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (0.29.12)\n","Requirement already satisfied: tensornets in /usr/local/lib/python3.6/dist-packages (0.4.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"92sknewTR4-f","colab_type":"text"},"source":["En el siguiente ejemplo se muestra como cargar una arquitectura pre-entrenada para usarla con un dato nuevo."]},{"cell_type":"code","metadata":{"id":"0AfL_3lrFPNU","colab_type":"code","outputId":"28f55aa1-e3e1-4856-c926-784160812d3f","executionInfo":{"status":"ok","timestamp":1563426715699,"user_tz":300,"elapsed":29118,"user":{"displayName":"Daniel Alexander Cano Cuartas","photoUrl":"https://lh5.googleusercontent.com/-4fBWDm-PEGk/AAAAAAAAAAI/AAAAAAAABLI/KrZmtZcFWbs/s64/photo.jpg","userId":"08439480702110199597"}},"colab":{"base_uri":"https://localhost:8080/","height":326}},"source":["\n","import tensorflow as tf\n","import tensornets as nets\n","\n","\n","tf.reset_default_graph()\n","\n","inputs = tf.placeholder(tf.float32, [None, 250, 250, 3])\n","model = nets.VGG19(inputs)\n","assert isinstance(model, tf.Tensor)\n","\n","img = nets.utils.load_img('usb_prueba.jpg', target_size=256, crop_size=250)\n","assert img.shape == (1, 250, 250, 3)\n","\n","with tf.Session() as sess:\n","    img = model.preprocess(img)  # equivalent to img = nets.preprocess(model, img)\n","    sess.run(model.pretrained())  # equivalent to nets.pretrained(model)\n","    preds = sess.run(model, {inputs: img})\n","\n","print(nets.utils.decode_predictions(preds, top=2)[0])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0718 05:11:28.613526 140238874142592 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensornets/utils.py:238: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","W0718 05:11:28.821262 140238874142592 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensornets/utils.py:277: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n","\n","W0718 05:11:29.286450 140238874142592 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","W0718 05:11:29.826313 140238874142592 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensornets/utils.py:246: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0718 05:11:29.828287 140238874142592 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensornets/utils.py:125: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://github.com/taehoonlee/deep-learning-models/releases/download/vgg/vgg19_weights_tf_dim_ordering_tf_kernels.h5\n","574717952/574710816 [==============================] - 17s 0us/step\n"],"name":"stdout"},{"output_type":"stream","text":["W0718 05:11:47.537585 140238874142592 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensornets/utils.py:130: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["[('n03777754', 'modem', 0.14609194), ('n03494278', 'harmonica', 0.14570944)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J3-QKKHVSUCl","colab_type":"text"},"source":["Pero si lo que queremos es usar una salida intermedia de la arquitectura preentrenada para entrenar el resto con nuestros datos seguimos el siguiente procedimiento usando las funciones  get_middles() y get_outputs():"]},{"cell_type":"code","metadata":{"id":"geUiy_FUSu02","colab_type":"code","outputId":"fb2f213b-fd45-4710-a66d-b1f706f5dc22","executionInfo":{"status":"ok","timestamp":1563426836510,"user_tz":300,"elapsed":6133,"user":{"displayName":"Daniel Alexander Cano Cuartas","photoUrl":"https://lh5.googleusercontent.com/-4fBWDm-PEGk/AAAAAAAAAAI/AAAAAAAABLI/KrZmtZcFWbs/s64/photo.jpg","userId":"08439480702110199597"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import tensorflow as tf\n","import tensornets as nets\n","\n","tf.reset_default_graph()\n","inputs = tf.placeholder(tf.float32, [None, 250, 250, 3])\n","model = nets.VGG19(inputs)\n","assert isinstance(model, tf.Tensor)\n","\n","img = nets.utils.load_img('usb_prueba.jpg', target_size=256, crop_size=250)\n","assert img.shape == (1, 250, 250, 3)\n","\n","with tf.Session() as sess:\n","    img = model.preprocess(img)\n","    sess.run(model.pretrained())\n","    middles = sess.run(model.get_middles(), {inputs: img})\n","    outputs = sess.run(model.get_outputs(), {inputs: img})\n","    \n","model.print_summary()\n","\n","model.print_middles()\n","assert middles[0].shape == (1, 62, 62, 256)\n","assert middles[-1].shape == (1, 15, 15, 512)\n","\n","model.print_outputs()"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Scope: vgg19\n","Total layers: 19\n","Total weights: 38\n","Total parameters: 143,667,240\n","Scope: vgg19\n","conv3/1/Relu:0 (?, 62, 62, 256)\n","conv3/2/Relu:0 (?, 62, 62, 256)\n","conv3/3/Relu:0 (?, 62, 62, 256)\n","conv3/4/Relu:0 (?, 62, 62, 256)\n","conv4/1/Relu:0 (?, 31, 31, 512)\n","conv4/2/Relu:0 (?, 31, 31, 512)\n","conv4/3/Relu:0 (?, 31, 31, 512)\n","conv4/4/Relu:0 (?, 31, 31, 512)\n","conv5/1/Relu:0 (?, 15, 15, 512)\n","conv5/2/Relu:0 (?, 15, 15, 512)\n","conv5/3/Relu:0 (?, 15, 15, 512)\n","conv5/4/Relu:0 (?, 15, 15, 512)\n","Scope: vgg19\n","conv1/1/conv/BiasAdd:0 (?, 250, 250, 64)\n","conv1/1/Relu:0 (?, 250, 250, 64)\n","conv1/2/conv/BiasAdd:0 (?, 250, 250, 64)\n","conv1/2/Relu:0 (?, 250, 250, 64)\n","conv1/pool/MaxPool:0 (?, 125, 125, 64)\n","conv2/1/conv/BiasAdd:0 (?, 125, 125, 128)\n","conv2/1/Relu:0 (?, 125, 125, 128)\n","conv2/2/conv/BiasAdd:0 (?, 125, 125, 128)\n","conv2/2/Relu:0 (?, 125, 125, 128)\n","conv2/pool/MaxPool:0 (?, 62, 62, 128)\n","conv3/1/conv/BiasAdd:0 (?, 62, 62, 256)\n","conv3/1/Relu:0 (?, 62, 62, 256)\n","conv3/2/conv/BiasAdd:0 (?, 62, 62, 256)\n","conv3/2/Relu:0 (?, 62, 62, 256)\n","conv3/3/conv/BiasAdd:0 (?, 62, 62, 256)\n","conv3/3/Relu:0 (?, 62, 62, 256)\n","conv3/4/conv/BiasAdd:0 (?, 62, 62, 256)\n","conv3/4/Relu:0 (?, 62, 62, 256)\n","conv3/pool/MaxPool:0 (?, 31, 31, 256)\n","conv4/1/conv/BiasAdd:0 (?, 31, 31, 512)\n","conv4/1/Relu:0 (?, 31, 31, 512)\n","conv4/2/conv/BiasAdd:0 (?, 31, 31, 512)\n","conv4/2/Relu:0 (?, 31, 31, 512)\n","conv4/3/conv/BiasAdd:0 (?, 31, 31, 512)\n","conv4/3/Relu:0 (?, 31, 31, 512)\n","conv4/4/conv/BiasAdd:0 (?, 31, 31, 512)\n","conv4/4/Relu:0 (?, 31, 31, 512)\n","conv4/pool/MaxPool:0 (?, 15, 15, 512)\n","conv5/1/conv/BiasAdd:0 (?, 15, 15, 512)\n","conv5/1/Relu:0 (?, 15, 15, 512)\n","conv5/2/conv/BiasAdd:0 (?, 15, 15, 512)\n","conv5/2/Relu:0 (?, 15, 15, 512)\n","conv5/3/conv/BiasAdd:0 (?, 15, 15, 512)\n","conv5/3/Relu:0 (?, 15, 15, 512)\n","conv5/4/conv/BiasAdd:0 (?, 15, 15, 512)\n","conv5/4/Relu:0 (?, 15, 15, 512)\n","conv5/pool/MaxPool:0 (?, 7, 7, 512)\n","flatten/flatten/Reshape:0 (?, 25088)\n","fc6/BiasAdd:0 (?, 4096)\n","relu6:0 (?, 4096)\n","drop6/Identity:0 (?, 4096)\n","fc7/BiasAdd:0 (?, 4096)\n","relu7:0 (?, 4096)\n","drop7/Identity:0 (?, 4096)\n","logits/BiasAdd:0 (?, 1000)\n","probs:0 (?, 1000)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q_O6Dbx2zrCz","colab_type":"text"},"source":["los valores almacenados en middles o en outputs, son salidas en capas intermedias de la arquitectura, y estas salidas se pueden usar como entradas para capas definidas por nosotros. Estas capas puede ser solo el MLP o Full conected al final de la arquitectura..."]},{"cell_type":"code","metadata":{"id":"RTIehYBRGCbI","colab_type":"code","colab":{}},"source":["import h5py\n","import matplotlib.pyplot as plt\n","import cv2\n","import numpy as np\n","import tensorflow as tf\n","import tensornets as nets\n","\n","\n","tf.reset_default_graph()\n","\n","# Parámetros para el entrenamiento\n","training_epochs=10\n","batch_size=10  \n","learning_rate = 0.001\n","display_step=1\n","\n","# Parametros de la red neuronal\n","n_hidden_1 = 1024 # 1st layer number of neurons\n","n_hidden_2 = 256 # 2nd layer number of neurons\n","n_input = 2048 # data input (feature shape ?,7,7,2048)\n","n_classes = 7 # total classes (4 signs)\n","\n","X = tf.placeholder(tf.float32, [None, 64, 64, 3])\n","X2 = tf.placeholder(tf.float32, [None, 2, 2, 2048])\n","Y = tf.placeholder(\"float\", [None, n_classes])\n","\n","# Para cargar el modelo con la libreria tensornets\n","model = nets.ResNet101(X)\n","assert isinstance(model, tf.Tensor)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6GhvEEWy0T0M","colab_type":"code","outputId":"b66527bd-faa1-4da2-a1b9-8c1460fe7375","executionInfo":{"status":"ok","timestamp":1563426887111,"user_tz":300,"elapsed":1855,"user":{"displayName":"Daniel Alexander Cano Cuartas","photoUrl":"https://lh5.googleusercontent.com/-4fBWDm-PEGk/AAAAAAAAAAI/AAAAAAAABLI/KrZmtZcFWbs/s64/photo.jpg","userId":"08439480702110199597"}},"colab":{"base_uri":"https://localhost:8080/","height":456}},"source":["# Leer de una base de datos H5\n","with h5py.File('dataset_office.h5','r') as h5data:\n","    ls=list(h5data.keys())\n","    print(ls)\n","    train_data=np.array(h5data.get('train_img')[:])\n","    train_labels=np.array(h5data.get('train_labels')[:])\n","    test_data=np.array(h5data.get('test_img')[:])\n","    test_labels=np.array(h5data.get('test_labels')[:])\n","\n","print(train_data.shape)    \n","print(train_labels.shape) \n","print(test_data.shape)    \n","print(test_labels.shape)  \n","\n","# Funcion para cambiar de tamaño las imágenes para adpatarlas a la arquitectura\n","def resize_np (np_array):\n","    resized=[]\n","    for i in list(np_array):\n","        larger=cv2.resize(i,(64,64))\n","        resized.append(np.array(larger))\n","    return (np.array(resized).astype(np.float32))\n","\n","# Funcion para cambiar labels a onehot\n","def one_hot_transformation(labels,n_classes):\n","    samples=labels.size\n","    one_hot_labels=np.zeros((samples,n_classes))\n","    for i in range(samples):\n","        one_hot_labels[i,labels[i]]=1\n","    return(one_hot_labels)\n","\n","X_train=resize_np(train_data)\n","print(X_train.shape)\n","X_test=resize_np(test_data)\n","print(X_test.shape)\n","Y_train=one_hot_transformation(train_labels,n_classes)\n","print(Y_train.shape)\n","Y_test=one_hot_transformation(test_labels,n_classes)\n","print(Y_test.shape)\n","\n","# mostrar un ejemplo\n","plt.imshow(X_train[0])"],"execution_count":14,"outputs":[{"output_type":"stream","text":["['test_img', 'test_labels', 'train_img', 'train_labels', 'train_mean', 'val_img', 'val_labels']\n","(6567, 64, 64, 3)\n","(6567,)\n","(2189, 64, 64, 3)\n","(2189,)\n","(6567, 64, 64, 3)\n","(2189, 64, 64, 3)\n"],"name":"stdout"},{"output_type":"stream","text":["W0718 05:14:46.260987 140238874142592 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"stream","text":["(6567, 7)\n","(2189, 7)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f893f887cc0>"]},"metadata":{"tags":[]},"execution_count":14},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"REFZ0Rk_SKAq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"fcfd07ca-6f23-4ef8-f441-9a1f23befd16","executionInfo":{"status":"ok","timestamp":1563426896282,"user_tz":300,"elapsed":1192,"user":{"displayName":"Daniel Alexander Cano Cuartas","photoUrl":"https://lh5.googleusercontent.com/-4fBWDm-PEGk/AAAAAAAAAAI/AAAAAAAABLI/KrZmtZcFWbs/s64/photo.jpg","userId":"08439480702110199597"}}},"source":["# Declaración de los pesos y los bias (weight & bias)\n","weights = {'h1': tf.Variable(tf.truncated_normal([n_input, n_hidden_1],stddev=0.1)),\n","           'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2],stddev=0.1)),\n","           'out': tf.Variable(tf.truncated_normal([n_hidden_2, n_classes],stddev=0.1))\n","          }\n","biases = {'b1': tf.Variable(tf.truncated_normal([n_hidden_1],stddev=0.1)),\n","          'b2': tf.Variable(tf.truncated_normal([n_hidden_2],stddev=0.1)),\n","          'out': tf.Variable(tf.truncated_normal([n_classes],stddev=0.1))\n","         }\n","\n","# Definición del perceptrón multicapa\n","def multilayer_perceptron(x):\n","    pool = tf.nn.avg_pool(x, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding='VALID')\n","    flat=tf.layers.flatten(pool)\n","    layer_1 = tf.add(tf.matmul(flat, weights['h1']), biases['b1']) # Hidden fully connected layer with 256 neurons\n","    relu_1=tf.nn.relu(layer_1)\n","    layer_2 = tf.add(tf.matmul(relu_1, weights['h2']), biases['b2']) # Hidden fully connected layer with 256 neurons\n","    relu_2=tf.nn.relu(layer_2)\n","    out_layer = tf.matmul(relu_2, weights['out']) + biases['out'] # Output fully connected layer with a neuron for each class\n","    return out_layer\n","# Declarar la operación que aplica el MLP usando la información de entrada\n","logits = multilayer_perceptron(X2)\n","# Declarar las operaciónes que establecen la funcion de perdida y optimización \n","# para el entrenamiento.\n","loss_op = tf.losses.softmax_cross_entropy(\n","    onehot_labels=Y,\n","    logits=logits,\n","    weights=1.0,\n","    scope=None,\n","    loss_collection=tf.GraphKeys.LOSSES,\n","    reduction=tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS\n",")\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","train_op = optimizer.minimize(loss_op)\n","# Initializing the variables\n","init = tf.global_variables_initializer()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["W0718 05:14:55.750278 140238874142592 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"XL33mzkuRDIH","colab_type":"code","outputId":"29bd6d66-ca20-4816-fda3-da6243f1e2b9","executionInfo":{"status":"ok","timestamp":1563427030941,"user_tz":300,"elapsed":130642,"user":{"displayName":"Daniel Alexander Cano Cuartas","photoUrl":"https://lh5.googleusercontent.com/-4fBWDm-PEGk/AAAAAAAAAAI/AAAAAAAABLI/KrZmtZcFWbs/s64/photo.jpg","userId":"08439480702110199597"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["with tf.Session() as sess:\n","    sess.run(init)\n","    sess.run(model.pretrained())  # equivalent to nets.pretrained(model)\n","    for epoch in range(training_epochs):\n","        avg_cost = 0.\n","        #obtiene el numero de grupos en que queda dividida la base de datos\n","        total_batch = int(Y_train.shape[0]/batch_size) \n","        # ciclo para entrenar con cada grupo de datos\n","        losses=[]\n","        for i in range(total_batch-1):\n","            batch_x= X_train[i*batch_size:(i+1)*batch_size]\n","            batch_y= Y_train[i*batch_size:(i+1)*batch_size]\n","            features = model.preprocess(batch_x)\n","            features = sess.run(model.get_middles(), {X: features})[-1]\n","            \n","            # Correr la funcion de perdida y la operacion de optimización \n","            # con la respectiva alimentación del placeholder\n","            _,c =sess.run([train_op, loss_op],feed_dict={X2:features,Y:batch_y})\n","            # Promedio de resultados de  = c_api.TF_FinishOperation(op_desc)la funcion de pérdida\n","            losses.append(c)\n","            avg_cost += c / total_batch\n","        # Mostrar el resultado del entrenamiento por grupos\n","        if epoch % display_step == 0:\n","            print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(avg_cost))\n","    print(\"Optimization Finished!\")"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Epoch: 0001 cost=2.001786614\n","Epoch: 0002 cost=0.406086255\n","Epoch: 0003 cost=0.266614193\n","Epoch: 0004 cost=0.236453812\n","Epoch: 0005 cost=0.223594277\n","Epoch: 0006 cost=0.172509004\n","Epoch: 0007 cost=0.264483118\n","Epoch: 0008 cost=0.183351425\n","Epoch: 0009 cost=0.102717602\n","Epoch: 0010 cost=0.203634878\n","Optimization Finished!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uYQkYRA3H_YY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}